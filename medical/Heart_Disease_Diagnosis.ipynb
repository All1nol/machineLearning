{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Diagnosis using Neural Networks\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading and exploring medical tabular data\n",
    "2. Data preprocessing and feature engineering\n",
    "3. **Data Augmentation using SMOTE** (for handling class imbalance)\n",
    "4. Building a Neural Network for classification\n",
    "5. Model evaluation and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation\n",
    "import pandas as pd  # for working with tabular data\n",
    "import numpy as np   # for numerical operations\n",
    "\n",
    "# Import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import libraries for preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Import SMOTE for data augmentation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Import Keras for neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the heart disease dataset\n",
    "df = pd.read_csv('heart_disease_data.csv')\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Descriptions:**\n",
    "- `age`: Age in years\n",
    "- `sex`: Sex (1 = male, 0 = female)\n",
    "- `cp`: Chest pain type (0-3)\n",
    "- `trestbps`: Resting blood pressure (mm Hg)\n",
    "- `chol`: Serum cholesterol (mg/dl)\n",
    "- `fbs`: Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)\n",
    "- `restecg`: Resting electrocardiographic results (0-2)\n",
    "- `thalach`: Maximum heart rate achieved\n",
    "- `exang`: Exercise induced angina (1 = yes, 0 = no)\n",
    "- `oldpeak`: ST depression induced by exercise\n",
    "- `slope`: Slope of the peak exercise ST segment (0-2)\n",
    "- `ca`: Number of major vessels (0-3) colored by fluoroscopy\n",
    "- `thal`: Thalassemia (1-3)\n",
    "- `target`: Diagnosis of heart disease (1 = disease, 0 = no disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset shape and info\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Check Class Distribution (Before Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "class_counts = df['target'].value_counts()\n",
    "print(\"Class Distribution (Before Augmentation):\")\n",
    "print(class_counts)\n",
    "print(f\"\\nClass 0 (No Disease): {class_counts[0]}\")\n",
    "print(f\"Class 1 (Disease): {class_counts[1]}\")\n",
    "print(f\"\\nImbalance Ratio: {class_counts[0] / class_counts[1]:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='target', data=df)\n",
    "plt.title('Class Distribution Before Data Augmentation')\n",
    "plt.xlabel('Target (0 = No Disease, 1 = Disease)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop('target', axis=1)  # All columns except target\n",
    "y = df['target']               # Only target column\n",
    "\n",
    "print(f\"Features Shape: {X.shape}\")\n",
    "print(f\"Target Shape: {y.shape}\")\n",
    "print(f\"\\nFeature Columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Ensure equal distribution of classes\n",
    ")\n",
    "\n",
    "print(f\"Training Set Size: {X_train.shape[0]}\")\n",
    "print(f\"Test Set Size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Feature Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features to have zero mean and unit variance\n",
    "# This is important for neural networks\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled Training Data Shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled Test Data Shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nFirst row of scaled data (first 5 features):\")\n",
    "print(X_train_scaled[0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: DATA AUGMENTATION using SMOTE\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)** is the tabular data equivalent of data augmentation for images.\n",
    "\n",
    "- **Image Augmentation**: Rotate, flip, zoom images to create new examples\n",
    "- **SMOTE**: Creates synthetic examples by interpolating between existing minority class samples\n",
    "\n",
    "This helps the model learn better by addressing class imbalance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to training data only\n",
    "print(\"Applying SMOTE Data Augmentation...\")\n",
    "print(\"BEFORE SMOTE:\")\n",
    "print(f\"Class 0 samples: {sum(y_train == 0)}\")\n",
    "print(f\"Class 1 samples: {sum(y_train == 1)}\")\n",
    "\n",
    "# Create SMOTE object\n",
    "# sampling_strategy='auto' means balance all classes\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "X_train_augmented, y_train_augmented = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nAFTER SMOTE:\")\n",
    "print(f\"Class 0 samples: {sum(y_train_augmented == 0)}\")\n",
    "print(f\"Class 1 samples: {sum(y_train_augmented == 1)}\")\n",
    "print(f\"\\nNew Training Set Size: {X_train_augmented.shape[0]}\")\n",
    "print(f\"Samples Added by SMOTE: {X_train_augmented.shape[0] - X_train_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution after augmentation\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y_train)\n",
    "plt.title('Before SMOTE')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_train_augmented)\n",
    "plt.title('After SMOTE (Augmented)')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Build the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential neural network\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "# We have 13 input features\n",
    "model.add(Dense(units=64, activation='relu', input_dim=13))\n",
    "model.add(Dropout(0.3))  # Dropout helps prevent overfitting\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "# Output layer (binary classification)\n",
    "# Sigmoid activation outputs probability between 0 and 1\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture Explanation:**\n",
    "\n",
    "1. **Input Layer**: 13 neurons (one for each feature)\n",
    "2. **Hidden Layer 1**: 64 neurons with ReLU activation\n",
    "3. **Dropout**: Randomly disables 30% of neurons (prevents overfitting)\n",
    "4. **Hidden Layer 2**: 32 neurons with ReLU activation\n",
    "5. **Dropout**: Randomly disables 20% of neurons\n",
    "6. **Hidden Layer 3**: 16 neurons with ReLU activation\n",
    "7. **Output Layer**: 1 neuron with Sigmoid (outputs probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on augmented data\n",
    "history = model.fit(\n",
    "    X_train_augmented, \n",
    "    y_train_augmented,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Visualize Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probability to class\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Disease', 'Disease']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks([0.5, 1.5], ['No Disease', 'Disease'])\n",
    "plt.yticks([0.5, 1.5], ['No Disease', 'Disease'])\n",
    "plt.show()\n",
    "\n",
    "print(f\"True Negatives (Correctly predicted No Disease): {cm[0,0]}\")\n",
    "print(f\"False Positives (Incorrectly predicted Disease): {cm[0,1]}\")\n",
    "print(f\"False Negatives (Incorrectly predicted No Disease): {cm[1,0]}\")\n",
    "print(f\"True Positives (Correctly predicted Disease): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 13: Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Predict for a new patient\n",
    "# Features: [age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]\n",
    "new_patient_1 = np.array([[52, 1, 0, 125, 212, 0, 1, 168, 0, 1.0, 2, 2, 3]])\n",
    "\n",
    "# Scale the new data using the same scaler\n",
    "new_patient_1_scaled = scaler.transform(new_patient_1)\n",
    "\n",
    "# Make prediction\n",
    "prediction_1 = model.predict(new_patient_1_scaled)\n",
    "print(f\"Patient 1 Risk Probability: {prediction_1[0][0] * 100:.2f}%\")\n",
    "if prediction_1[0][0] > 0.5:\n",
    "    print(\"Diagnosis: HIGH RISK of Heart Disease\")\n",
    "else:\n",
    "    print(\"Diagnosis: LOW RISK of Heart Disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Another patient\n",
    "new_patient_2 = np.array([[45, 0, 1, 130, 233, 0, 1, 198, 0, 0.2, 1, 0, 2]])\n",
    "\n",
    "# Scale and predict\n",
    "new_patient_2_scaled = scaler.transform(new_patient_2)\n",
    "prediction_2 = model.predict(new_patient_2_scaled)\n",
    "\n",
    "print(f\"Patient 2 Risk Probability: {prediction_2[0][0] * 100:.2f}%\")\n",
    "if prediction_2[0][0] > 0.5:\n",
    "    print(\"Diagnosis: HIGH RISK of Heart Disease\")\n",
    "else:\n",
    "    print(\"Diagnosis: LOW RISK of Heart Disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 14: Compare With vs Without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model WITHOUT SMOTE for comparison\n",
    "print(\"Training model WITHOUT data augmentation...\")\n",
    "\n",
    "model_no_smote = Sequential()\n",
    "model_no_smote.add(Dense(units=64, activation='relu', input_dim=13))\n",
    "model_no_smote.add(Dropout(0.3))\n",
    "model_no_smote.add(Dense(units=32, activation='relu'))\n",
    "model_no_smote.add(Dropout(0.2))\n",
    "model_no_smote.add(Dense(units=16, activation='relu'))\n",
    "model_no_smote.add(Dense(units=1, activation='sigmoid'))\n",
    "model_no_smote.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_no_smote = model_no_smote.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Compare accuracies\n",
    "accuracy_with_smote = model.evaluate(X_test_scaled, y_test, verbose=0)[1]\n",
    "accuracy_without_smote = model_no_smote.evaluate(X_test_scaled, y_test, verbose=0)[1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON: With vs Without SMOTE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"WITH SMOTE:     {accuracy_with_smote * 100:.2f}% accuracy\")\n",
    "print(f\"WITHOUT SMOTE:  {accuracy_without_smote * 100:.2f}% accuracy\")\n",
    "print(f\"Improvement:    {(accuracy_with_smote - accuracy_without_smote) * 100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. ✅ Loaded and explored medical tabular data\n",
    "2. ✅ Preprocessed features using StandardScaler\n",
    "3. ✅ **Applied SMOTE data augmentation** to handle class imbalance\n",
    "4. ✅ Built a neural network with dropout for regularization\n",
    "5. ✅ Trained and evaluated the model\n",
    "6. ✅ Made predictions on new patient data\n",
    "7. ✅ Compared performance with and without data augmentation\n",
    "\n",
    "**Key Takeaways:**\n",
    "- **SMOTE** creates synthetic samples to balance classes (similar to image augmentation)\n",
    "- **Feature scaling** is crucial for neural network performance\n",
    "- **Dropout** helps prevent overfitting\n",
    "- **Data augmentation** can improve model performance on imbalanced datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
